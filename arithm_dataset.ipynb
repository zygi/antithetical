{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Tuple\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/mnt/storage/hf_cache'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d96a9cbbc42b4d7a8b44f0aa13d6e1e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, Phi3ForCausalLM, LlamaForCausalLM\n",
    "from transformers.pipelines.text_generation import Chat\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "MODEL_NAME = \"NousResearch/Hermes-3-Llama-3.1-8B\"\n",
    "\n",
    "model: LlamaForCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    # \"microsoft/Phi-3.5-mini-instruct\", \n",
    "    device_map=\"cuda\", \n",
    "    # torch_dtype=\"float32\", \n",
    "    torch_dtype=\"auto\",\n",
    ")\n",
    "model.eval()\n",
    "# model = torch.compile(model, dynamic=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'OptimizedModule' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    # batch_size=8,\n",
    ")\n",
    "\n",
    "\n",
    "EXAMPLE_DESCRIPTION = \"\"\"For example if the question is \"\n",
    "What is 75 - 66? Is it\n",
    "A: 0\n",
    "B: 9\n",
    "C: 3\n",
    "?\"\n",
    "you should output the string B, because 475 - 466 = 9.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Your goal is to answer a multiple-choice arithmetic question doing subtraction. It will be in a format\n",
    "\"What is <number> - <number>? Is it\n",
    "A: <digit1>\n",
    "B: <digit2>\n",
    "C: <digit3>\n",
    "?\"\n",
    "where <digit1>, <digit2>, and <digit3> are one digit numbers.\n",
    "\n",
    "You are expected to output only the uppercase letter of the correct answer: A, B, C. Output nothing else. {EXAMPLE_DESCRIPTION}\n",
    "\n",
    "Remember to not succumb to answer order bias. Don't automatically choose the first answer.\n",
    "With that in mind, here is the question:\n",
    "\n",
    "--------------------------------\n",
    "What is {NUMBER} - {NUMBER_TO_SUBTRACT}? Is it\n",
    "A: {ANSWER_A}\n",
    "B: {ANSWER_B}\n",
    "C: {ANSWER_C}\n",
    "?\n",
    "\"\"\"\n",
    "\n",
    "def generate_question(num_digits: int, rng: random.Random) -> Tuple[str, int]:\n",
    "    number = rng.randint(10**(num_digits - 1), 10**num_digits - 1)\n",
    "    difference = rng.randint(0,9)\n",
    "    number_to_subtract = number - difference\n",
    "    \n",
    "    answer_candidates = rng.sample([i for i in range(10) if i != number_to_subtract], 2) + [difference]\n",
    "    rng.shuffle(answer_candidates)\n",
    "    \n",
    "    # answers = [0, 1, 2]\n",
    "    # rng.shuffle(answers)\n",
    "    \n",
    "    correct_answer_index = answer_candidates.index(difference)\n",
    "    \n",
    "    return PROMPT_TEMPLATE.format(NUMBER=number, NUMBER_TO_SUBTRACT=number_to_subtract, ANSWER_A=answer_candidates[0], ANSWER_B=answer_candidates[1], ANSWER_C=answer_candidates[2],\n",
    "                                #   EXAMPLE_DESCRIPTION=EXAMPLE_DESCRIPTION\n",
    "                                  EXAMPLE_DESCRIPTION=\"\"\n",
    "                                  ), correct_answer_index\n",
    "\n",
    "\n",
    "\n",
    "test_rng = random.Random(42)\n",
    "examples = [generate_question(2, test_rng) for _ in range(1000)]\n",
    "\n",
    "# examples[:3]\n",
    "\n",
    "example_dialogs = [[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "] for prompt, _ in examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Your goal is to answer a multiple-choice arithmetic question doing subtraction. It will be in a format\n",
      "\"What is <number> - <number>? Is it\n",
      "A: <digit1>\n",
      "B: <digit2>\n",
      "C: <digit3>\n",
      "?\"\n",
      "where <digit1>, <digit2>, and <digit3> are one digit numbers.\n",
      "\n",
      "You are expected to output only the uppercase letter of the correct answer: A, B, C. Output nothing else. \n",
      "\n",
      "Remember to not succumb to answer order bias. Don't automatically choose the first answer.\n",
      "With that in mind, here is the question:\n",
      "\n",
      "--------------------------------\n",
      "What is 15 - 8? Is it\n",
      "A: 7\n",
      "B: 9\n",
      "C: 1\n",
      "?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(examples[7][0])\n",
    "# print(examples[6][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'generated_text': [{'role': 'system',\n",
       "     'content': 'You are a helpful AI assistant.'},\n",
       "    {'role': 'user',\n",
       "     'content': '\\nYour goal is to answer a multiple-choice arithmetic question doing subtraction. It will be in a format\\n\"What is <number> - <number>? Is it\\nA: <digit1>\\nB: <digit2>\\nC: <digit3>\\n?\"\\nwhere <digit1>, <digit2>, and <digit3> are one digit numbers.\\n\\nYou are expected to output only the uppercase letter of the correct answer: A, B, C. Output nothing else. \\n\\nRemember to not succumb to answer order bias. Don\\'t automatically choose the first answer.\\nWith that in mind, here is the question:\\n\\n--------------------------------\\nWhat is 91 - 90? Is it\\nA: 4\\nB: 1\\nC: 0\\n?\\n'},\n",
       "    {'role': 'assistant', 'content': 'A'}]}]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.tokenizer.pad_token_id = model.config.eos_token_id\n",
    "pipe(example_dialogs[:1], max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from transformers.models.llama.modeling_llama import LlamaDecoderLayer\n",
    "from torch import nn\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class NoiseInjectionConfig(BaseModel):\n",
    "    mode: Literal[\"none\", \"antipodal\", \"random\", \"random_mult\"] = \"none\"\n",
    "    noise_mean: float = 0.0\n",
    "    noise_std: float = 5e-1\n",
    "    # batch_size: int = 16\n",
    "\n",
    "NOISE_INJECTION_CONFIG: NoiseInjectionConfig | None = None\n",
    "    \n",
    "# NOISE_INJECTION_MODE: Literal[\"none\", \"antipodal\", \"random\", \"random_mult\"] = \"none\"\n",
    "\n",
    "class LlamaDecoderLayerWrapper(nn.Module):\n",
    "    def __init__(self, layer: LlamaDecoderLayer):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        ref_weight = self.layer.mlp.gate_proj.weight\n",
    "        # self.noise_mean = nn.Parameter(torch.tensor(noise_mean, device=ref_weight.device, dtype=ref_weight.dtype))\n",
    "        # self.noise_std = nn.Parameter(torch.tensor(noise_std, device=ref_weight.device, dtype=ref_weight.dtype))\n",
    "\n",
    "    def forward(self, hidden_states, *args, **kwargs):\n",
    "        batch_dim = hidden_states.shape[0]\n",
    "        \n",
    "        if NOISE_INJECTION_CONFIG is None:\n",
    "            return self.layer(hidden_states, *args, **kwargs)\n",
    "        \n",
    "        # print(hidden_states.mean(dim=(1, 2))[0])\n",
    "        if NOISE_INJECTION_CONFIG.mode==\"antipodal\":\n",
    "            # print(\"INJECTING NOISE AAA\")\n",
    "            assert batch_dim % 2 == 0\n",
    "            half_dim = batch_dim // 2\n",
    "            noise = torch.randn(half_dim, *hidden_states.shape[1:], device=hidden_states.device, dtype=hidden_states.dtype) * NOISE_INJECTION_CONFIG.noise_std + NOISE_INJECTION_CONFIG.noise_mean\n",
    "            noise = torch.cat([noise, -noise], dim=0)\n",
    "            new_hidden_states = hidden_states + noise\n",
    "        elif NOISE_INJECTION_CONFIG.mode==\"random\":\n",
    "            # print(\"INJECTING NOISE BBB\")\n",
    "            noise = torch.randn(hidden_states.shape, device=hidden_states.device, dtype=hidden_states.dtype) * NOISE_INJECTION_CONFIG.noise_std + NOISE_INJECTION_CONFIG.noise_mean\n",
    "            new_hidden_states = hidden_states + noise\n",
    "        elif NOISE_INJECTION_CONFIG.mode==\"random_mult\":\n",
    "            noise = torch.randn(hidden_states.shape, device=hidden_states.device, dtype=hidden_states.dtype) * NOISE_INJECTION_CONFIG.noise_std + NOISE_INJECTION_CONFIG.noise_mean\n",
    "            new_hidden_states = hidden_states * (1 + noise)\n",
    "        else:\n",
    "            new_hidden_states = hidden_states\n",
    "        \n",
    "        # print(((new_hidden_states - hidden_states).abs() / (hidden_states.abs() + 1e-10)).mean())\n",
    "        return self.layer(new_hidden_states, *args, **kwargs)\n",
    "\n",
    "\n",
    "LAYER = 30\n",
    "for l in range(LAYER, len(model.model.layers)):\n",
    "    while str(type(model.model.layers[l])) == \"<class '__main__.LlamaDecoderLayerWrapper'>\":\n",
    "        model.model.layers[l] = model.model.layers[l].layer\n",
    "    else:\n",
    "        break\n",
    "model.model.layers[LAYER] = LlamaDecoderLayerWrapper(model.model.layers[LAYER])\n",
    "import transformers.tokenization_utils_base\n",
    "\n",
    "def preprocess_manual(messages: list[list[dict]]) -> transformers.tokenization_utils_base.BatchEncoding:\n",
    "    one_by_one = [pipe.preprocess(Chat(message), return_tensors=\"pt\") for message in messages]\n",
    "    # stack\n",
    "    return transformers.tokenization_utils_base.BatchEncoding(\n",
    "        {k: torch.cat([t[k] for t in one_by_one], dim=0) for k in one_by_one[0].keys() if k != \"prompt_text\" }\n",
    "    )\n",
    "    \n",
    "with torch.no_grad():\n",
    "    inputs = preprocess_manual(example_dialogs).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 128256])\n",
      "torch.Size([1, 8, 144, 128])\n",
      "torch.Size([4, 128256])\n",
      "tensor([[ 0.0312,  0.0625,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0625, -0.0625, -0.0625,  ..., -0.0156, -0.0156, -0.0156],\n",
      "        [-0.0938,  0.0000,  0.0625,  ...,  0.0625,  0.0625,  0.0625],\n",
      "        [-0.0312,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "def find_kvcacheable_size(input_ids: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.all(input_ids[0] == input_ids, dim=0).int().argmin()\n",
    "\n",
    "find_kvcacheable_size(inputs['input_ids'])\n",
    "\n",
    "from transformers import DynamicCache\n",
    "\n",
    "\n",
    "# test\n",
    "@torch.no_grad()\n",
    "def test_kvcache():\n",
    "    test_num = 4\n",
    "    base_logits = model(**(inputs[:test_num])).logits[:, -1, :]\n",
    "    print(base_logits.shape)\n",
    "\n",
    "    kv_size = find_kvcacheable_size(inputs['input_ids'])\n",
    "    past_key_values = DynamicCache()\n",
    "    pkv = model(inputs['input_ids'][:1, :kv_size], past_key_values=past_key_values, use_cache=True)\n",
    "\n",
    "    print(pkv.past_key_values[0][0].shape)\n",
    "    pkv.past_key_values.batch_repeat_interleave(test_num)\n",
    "    new_logits = model(inputs['input_ids'][:test_num, kv_size:], past_key_values=pkv.past_key_values, use_cache=True).logits[:, -1, :]\n",
    "    print(new_logits.shape)\n",
    "    \n",
    "    print (new_logits - base_logits)\n",
    "\n",
    "test_kvcache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[32],\n",
      "        [33],\n",
      "        [34]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:14<00:00,  1.67it/s]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def batched_eval(fn, input_ids, *args, batch_size=8, use_tqdm=True, use_cache=True, **kwargs):\n",
    "    all_tensors = [input_ids] + list(args) + list(kwargs.values())\n",
    "    assert all(isinstance(t, torch.Tensor) for t in all_tensors), f\"All arguments must be tensors, got {all_tensors}\"\n",
    "    \n",
    "    bs1 = input_ids.shape[0]\n",
    "    assert all(t.shape[0] == bs1 for t in all_tensors), f\"All tensors must have the same batch size, got {all_tensors}\"\n",
    "    res = None\n",
    "    iter = range(0, bs1, batch_size) if not use_tqdm else tqdm(range(0, bs1, batch_size))\n",
    "    \n",
    "    # if use_cache:\n",
    "    #     kv_len = find_kvcacheable_size(input_ids)\n",
    "    #     kv_cache = DynamicCache()\n",
    "        \n",
    "    #     kv_cache = model(input_ids[:1, :kv_len], *[a[:1, :kv_len] for a in args], **{k: v[:1, :kv_len] for k, v in kwargs.items()}, past_key_values=kv_cache, use_cache=True).past_key_values\n",
    "        \n",
    "    for i in iter:\n",
    "        upto = min(i + batch_size, bs1)\n",
    "        # if use_cache:\n",
    "        #     kv_cache_copy = copy.deepcopy(kv_cache)\n",
    "        #     kv_cache_copy.batch_repeat_interleave(upto - i)\n",
    "        #     partial_res = fn(input_ids[i:upto, :kv_len], *[arg[i:upto, :kv_len] for arg in args], **{k: v[i:upto, :kv_len] for k, v in kwargs.items()}, past_key_values=kv_cache_copy, use_cache=True)\n",
    "        # else:\n",
    "        partial_res = fn(input_ids[i:upto], *[arg[i:upto] for arg in args], **{k: v[i:upto] for k, v in kwargs.items()})\n",
    "        if res is None:\n",
    "            res = torch.empty((bs1, *partial_res.shape[1:]), device=partial_res.device, dtype=partial_res.dtype)\n",
    "        res[i:upto] = partial_res\n",
    "    return res\n",
    "\n",
    "# torch.vmap(model, chunk_size=2)(**(inputs[:8]))\n",
    "    \n",
    "# inputs[:8]\n",
    "    \n",
    "with torch.no_grad():\n",
    "    tokens_of_interest = tokenizer(['A', 'B', 'C'], return_tensors=\"pt\", add_special_tokens=False)['input_ids'].to('cuda')\n",
    "    print(tokens_of_interest)\n",
    "    # if tokens_of_interest\n",
    "    \n",
    "    if tokens_of_interest.shape != (3, 1):\n",
    "        raise ValueError(f\"Expected tokens_of_interest to have shape (3, 1), got {tokens_of_interest.shape}\")\n",
    "    tokens_of_interest = tokens_of_interest[:, 0]\n",
    "    \n",
    "    def inner(input_ids, attention_mask, past_key_values = None, **kwargs):\n",
    "        # expand by 2\n",
    "        N_REPEATS = 4 if NOISE_INJECTION_CONFIG is not None else 1\n",
    "        input_ids = input_ids.repeat(N_REPEATS, 1)\n",
    "        attention_mask = attention_mask.repeat(N_REPEATS, 1)\n",
    "        if past_key_values is not None:\n",
    "            past_key_values = past_key_values.batch_repeat_interleave(N_REPEATS)\n",
    "        # with torch.amp.autocast('cuda'):\n",
    "        \n",
    "        \n",
    "        \n",
    "        res = model.forward(input_ids, attention_mask, past_key_values=past_key_values, **kwargs).logits[:, -1, tokens_of_interest]\n",
    "        # average the created pairs\n",
    "        if N_REPEATS > 1:\n",
    "            res = res.view(N_REPEATS, res.shape[0] // N_REPEATS, *res.shape[1:]).mean(dim=0)\n",
    "        # print(res.shape)\n",
    "        return res\n",
    "        \n",
    "    NOISE_INJECTION_CONFIG = NoiseInjectionConfig(mode=\"random\", noise_mean=0.0, noise_std=5e-1)\n",
    "    # NOISE_INJECTION_CONFIG = None\n",
    "    res = batched_eval(inner, **inputs, batch_size=8, use_cache=False)\n",
    "    NOISE_INJECTION_CONFIG = None\n",
    "\n",
    "    # print(model.generate(**inputs[:20], max_new_tokens=50))#[:, inputs['input_ids'][0].shape[-1]:])\n",
    "\n",
    "# tokenizer.batch_decode(model.generate(**preprocess_manual(example_dialogs[:10]).to('cuda'), max_new_tokens=50))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([25.7500, 22.7500, 20.3750], device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([25.7500, 22.7500, 20.3750], device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor(0.5685), tensor(0.5813), tensor(0.1637)], tensor(0.4590))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "answers = torch.tensor([v for _, v in examples[:len(res)]])\n",
    "is_correct = answers == res.argmax(dim=-1).to('cpu')\n",
    "is_correct_given_position = [is_correct[answers == i].float().mean() for i in range(3)]\n",
    "\n",
    "is_correct_given_position, is_correct.float().mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'logits'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m logits_of_answers \u001b[38;5;241m=\u001b[39m \u001b[43mres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, tokens_of_interest]\n\u001b[1;32m      2\u001b[0m logits_of_answers\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'logits'"
     ]
    }
   ],
   "source": [
    "\n",
    "logits_of_answers = res.logits[:, -1, tokens_of_interest]\n",
    "logits_of_answers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[350],\n",
       "         [319],\n",
       "         [350],\n",
       "         [315],\n",
       "         [319],\n",
       "         [350],\n",
       "         [315],\n",
       "         [350],\n",
       "         [350],\n",
       "         [350]], device='cuda:0'),\n",
       " {'input_ids': [[319], [350], [315]], 'attention_mask': [[1], [1], [1]]})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# res.logits[:, -1:, :].argmax(dim=-1),\\\n",
    "#     tokenizer(['A', 'B', 'C'])\n",
    "\n",
    "# tokenizer.batch_decode(res.logits[:, -1:, :].argmax(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
