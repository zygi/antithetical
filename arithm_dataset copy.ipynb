{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Tuple\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/mnt/storage/hf_cache'\n",
    "\n",
    "\n",
    "EXAMPLE_DESCRIPTION = \"\"\"For example if the question is \"\n",
    "What is 13 % 3? Is it\n",
    "A: 1\n",
    "B: 0\n",
    "C: 2\n",
    "?\"\n",
    "you should output the string A, because 13 = 3 * 4 + 1, so the remainder is 1.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Your goal is to answer a multiple-choice arithmetic question. It will be in a format\n",
    "\"What is <number> % 3? Is it\n",
    "A: <digit1>\n",
    "B: <digit2>\n",
    "C: <digit3>\n",
    "?\"\n",
    "where <digit1>, <digit2>, and <digit3> are digits 0 through 2.\n",
    "\n",
    "You are expected to output only the uppercaseletter of the correct answer: A, B, C. Output nothing else. {EXAMPLE_DESCRIPTION}\n",
    "\n",
    "With that in mind, here is the question:\n",
    "\n",
    "--------------------------------\n",
    "What is {NUMBER} % {MODULO}? Is it\n",
    "A: {ANSWER_A}\n",
    "B: {ANSWER_B}\n",
    "C: {ANSWER_C}\n",
    "?\n",
    "\"\"\"\n",
    "\n",
    "def generate_question(num_digits: int, rng: random.Random) -> Tuple[str, int]:\n",
    "    number = rng.randint(10**(num_digits - 1), 10**num_digits - 1)\n",
    "    answers = [0, 1, 2]\n",
    "    rng.shuffle(answers)\n",
    "    \n",
    "    correct_answer_index = answers.index(number % 3)\n",
    "    \n",
    "    return PROMPT_TEMPLATE.format(NUMBER=number, MODULO=3, ANSWER_A=answers[0], ANSWER_B=answers[1], ANSWER_C=answers[2], EXAMPLE_DESCRIPTION=EXAMPLE_DESCRIPTION), correct_answer_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9c7d562f21e4e08a2ae65459cb83334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, Phi3ForCausalLM, LlamaForCausalLM\n",
    "from transformers.pipelines.text_generation import Chat\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model: LlamaForCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3.1-8B-Instruct\", \n",
    "    # \"microsoft/Phi-3.5-mini-instruct\", \n",
    "    device_map=\"cuda\", \n",
    "    # torch_dtype=\"float32\", \n",
    "    torch_dtype=\"auto\",\n",
    ")\n",
    "model.eval()\n",
    "# model = torch.compile(model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "test_rng = random.Random(42)\n",
    "examples = [generate_question(1, test_rng) for _ in range(1000)]\n",
    "\n",
    "# examples[:3]\n",
    "\n",
    "example_dialogs = [[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "] for prompt, _ in examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples[5][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  2.37it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import transformers.tokenization_utils_base\n",
    "\n",
    "def preprocess_manual(messages: list[list[dict]]) -> transformers.tokenization_utils_base.BatchEncoding:\n",
    "    one_by_one = [pipe.preprocess(Chat(message), return_tensors=\"pt\") for message in messages]\n",
    "    # stack\n",
    "    return transformers.tokenization_utils_base.BatchEncoding(\n",
    "        {k: torch.cat([t[k] for t in one_by_one], dim=0) for k in one_by_one[0].keys() if k != \"prompt_text\" }\n",
    "    )\n",
    "    \n",
    "\n",
    "@torch.no_grad()\n",
    "def batched_eval(fn, *args, batch_size=8, use_tqdm=True, **kwargs):\n",
    "    assert all(isinstance(arg, torch.Tensor) for arg in args), f\"All arguments must be tensors, got {args}\"\n",
    "    assert all(isinstance(arg, torch.Tensor) for arg in kwargs.values()), f\"All kwargs must be tensors, got {kwargs}\"\n",
    "    \n",
    "    bs1 = args[0].shape[0] if len(args) > 0 else list(kwargs.values())[0].shape[0]\n",
    "    assert all(arg.shape[0] == bs1 for arg in args), f\"All arguments must have the same batch size, got {args}\"\n",
    "    assert all(arg.shape[0] == bs1 for arg in kwargs.values()), f\"All kwargs must have the same batch size, got {kwargs}\"\n",
    "    res = None\n",
    "    iter = range(0, bs1, batch_size) if not use_tqdm else tqdm(range(0, bs1, batch_size))\n",
    "    for i in iter:\n",
    "        upto = min(i + batch_size, bs1)\n",
    "        partial_res = fn(*[arg[i:upto] for arg in args], **{k: v[i:upto] for k, v in kwargs.items()})\n",
    "        if res is None:\n",
    "            res = torch.empty((bs1, *partial_res.shape[1:]), device=partial_res.device, dtype=partial_res.dtype)\n",
    "        res[i:upto] = partial_res\n",
    "    return res\n",
    "\n",
    "# torch.vmap(model, chunk_size=2)(**(inputs[:8]))\n",
    "    \n",
    "# inputs[:8]\n",
    "    \n",
    "with torch.no_grad():\n",
    "    inputs = preprocess_manual(example_dialogs).to('cuda')\n",
    "    tokens_of_interest = tokenizer(['A', 'B', 'C'], return_tensors=\"pt\", add_special_tokens=False)['input_ids'].to('cuda')\n",
    "    if tokens_of_interest.shape != (3, 1):\n",
    "        raise ValueError(f\"Expected tokens_of_interest to have shape (3, 1), got {tokens_of_interest.shape}\")\n",
    "    tokens_of_interest = tokens_of_interest[:, 0]\n",
    "    \n",
    "    def inner(*args, **kwargs):\n",
    "        # with torch.amp.autocast('cuda'):\n",
    "        return model.forward(*args, **kwargs).logits[:, -1, tokens_of_interest]\n",
    "\n",
    "    res = batched_eval(inner, **inputs[:100], batch_size=16)\n",
    "\n",
    "\n",
    "# tokenizer.batch_decode(model.generate(**preprocess_manual(example_dialogs[:10]).to('cuda'), max_new_tokens=50))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[30.5000, 29.2500, 29.2500],\n",
       "        [30.2500, 28.7500, 29.5000],\n",
       "        [30.5000, 30.1250, 26.1250],\n",
       "        [30.8750, 29.7500, 28.6250],\n",
       "        [30.1250, 28.5000, 29.6250],\n",
       "        [31.2500, 30.2500, 26.5000],\n",
       "        [31.2500, 30.0000, 29.2500],\n",
       "        [31.2500, 30.1250, 27.7500],\n",
       "        [30.6250, 29.2500, 28.8750],\n",
       "        [31.3750, 31.0000, 27.1250],\n",
       "        [30.8750, 30.6250, 28.0000],\n",
       "        [30.7500, 30.2500, 28.5000],\n",
       "        [31.6250, 30.5000, 27.2500],\n",
       "        [31.3750, 29.7500, 26.7500],\n",
       "        [31.2500, 30.0000, 29.2500],\n",
       "        [31.0000, 30.5000, 27.2500],\n",
       "        [31.1250, 30.7500, 25.6250],\n",
       "        [30.1250, 28.5000, 29.6250],\n",
       "        [31.0000, 29.5000, 28.8750],\n",
       "        [31.0000, 30.2500, 28.3750],\n",
       "        [31.5000, 30.5000, 27.3750],\n",
       "        [31.6250, 28.5000, 26.7500],\n",
       "        [31.3750, 30.2500, 27.3750],\n",
       "        [30.5000, 30.1250, 26.1250],\n",
       "        [31.2500, 30.2500, 26.5000],\n",
       "        [31.6250, 28.5000, 26.7500],\n",
       "        [31.5000, 30.3750, 26.5000],\n",
       "        [30.7500, 28.6250, 30.1250],\n",
       "        [31.1250, 29.3750, 27.5000],\n",
       "        [31.0000, 29.5000, 28.8750],\n",
       "        [30.8750, 28.8750, 29.7500],\n",
       "        [31.3750, 30.8750, 27.1250],\n",
       "        [30.5000, 30.3750, 25.8750],\n",
       "        [31.3750, 30.2500, 27.3750],\n",
       "        [31.3750, 30.8750, 27.1250],\n",
       "        [30.7500, 28.6250, 30.1250],\n",
       "        [31.3750, 30.5000, 28.8750],\n",
       "        [31.0000, 27.5000, 29.2500],\n",
       "        [31.6250, 28.6250, 26.5000],\n",
       "        [31.0000, 30.2500, 28.3750],\n",
       "        [31.5000, 30.5000, 27.3750],\n",
       "        [30.8750, 29.1250, 29.2500],\n",
       "        [31.5000, 30.2500, 28.7500],\n",
       "        [31.0000, 30.2500, 28.3750],\n",
       "        [30.5000, 30.3750, 25.8750],\n",
       "        [31.5000, 30.3750, 26.5000],\n",
       "        [31.2500, 30.0000, 29.2500],\n",
       "        [31.2500, 30.2500, 26.5000],\n",
       "        [30.5000, 30.3750, 25.8750],\n",
       "        [31.2500, 30.2500, 26.5000],\n",
       "        [31.3750, 31.0000, 27.1250],\n",
       "        [31.0000, 30.2500, 28.3750],\n",
       "        [30.2500, 28.7500, 29.5000],\n",
       "        [30.5000, 29.2500, 29.2500],\n",
       "        [30.5000, 30.1250, 26.1250],\n",
       "        [31.6250, 28.6250, 26.5000],\n",
       "        [31.2500, 30.0000, 29.2500],\n",
       "        [31.5000, 30.5000, 27.3750],\n",
       "        [30.7500, 30.3750, 26.0000],\n",
       "        [31.6250, 30.5000, 28.2500],\n",
       "        [31.5000, 30.5000, 27.3750],\n",
       "        [30.1250, 28.5000, 29.6250],\n",
       "        [30.7500, 30.2500, 28.5000],\n",
       "        [30.8750, 29.1250, 29.2500],\n",
       "        [31.0000, 27.5000, 29.2500],\n",
       "        [30.2500, 28.7500, 29.5000],\n",
       "        [30.7500, 28.6250, 30.1250],\n",
       "        [31.6250, 30.7500, 26.7500],\n",
       "        [30.7500, 30.3750, 26.0000],\n",
       "        [31.2500, 30.2500, 29.2500],\n",
       "        [30.8750, 29.7500, 28.6250],\n",
       "        [31.0000, 30.5000, 27.2500],\n",
       "        [31.6250, 30.5000, 27.2500],\n",
       "        [31.3750, 30.7500, 26.6250],\n",
       "        [30.5000, 29.2500, 29.2500],\n",
       "        [30.7500, 28.6250, 30.1250],\n",
       "        [30.5000, 29.5000, 29.3750],\n",
       "        [30.5000, 30.3750, 25.8750],\n",
       "        [31.3750, 30.3750, 28.8750],\n",
       "        [30.7500, 30.2500, 28.5000],\n",
       "        [31.5000, 29.7500, 25.8750],\n",
       "        [30.1250, 28.5000, 29.6250],\n",
       "        [31.3750, 30.6250, 28.7500],\n",
       "        [30.8750, 30.6250, 28.0000],\n",
       "        [30.5000, 30.3750, 25.8750],\n",
       "        [31.3750, 30.6250, 28.7500],\n",
       "        [31.6250, 30.5000, 28.2500],\n",
       "        [29.8750, 25.8750, 29.6250],\n",
       "        [30.2500, 28.7500, 29.5000],\n",
       "        [30.7500, 28.6250, 30.1250],\n",
       "        [31.3750, 30.7500, 26.6250],\n",
       "        [31.5000, 29.7500, 25.8750],\n",
       "        [30.1250, 24.7500, 30.3750],\n",
       "        [30.8750, 30.6250, 28.0000],\n",
       "        [30.7500, 30.3750, 26.0000],\n",
       "        [31.0000, 30.2500, 28.3750],\n",
       "        [31.3750, 30.6250, 28.5000],\n",
       "        [31.2500, 30.2500, 27.7500],\n",
       "        [31.6250, 28.6250, 26.3750],\n",
       "        [31.5000, 30.8750, 28.0000]], device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3300)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.tensor([v for _, v in examples[:len(res)]]) == res.argmax(dim=-1).to('cpu')).float().mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[45.2500, 52.5000, 46.5000],\n",
       "        [54.5000, 50.2500, 46.0000],\n",
       "        [44.5000, 52.5000, 48.0000],\n",
       "        [47.7500, 54.0000, 54.7500],\n",
       "        [51.0000, 40.7500, 42.2500],\n",
       "        [44.5000, 52.2500, 47.2500],\n",
       "        [53.7500, 47.0000, 56.0000],\n",
       "        [49.7500, 52.7500, 41.7500],\n",
       "        [34.2500, 59.7500, 48.5000],\n",
       "        [46.5000, 51.0000, 48.2500]], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "logits_of_answers = res.logits[:, -1, tokens_of_interest]\n",
    "logits_of_answers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[350],\n",
       "         [319],\n",
       "         [350],\n",
       "         [315],\n",
       "         [319],\n",
       "         [350],\n",
       "         [315],\n",
       "         [350],\n",
       "         [350],\n",
       "         [350]], device='cuda:0'),\n",
       " {'input_ids': [[319], [350], [315]], 'attention_mask': [[1], [1], [1]]})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# res.logits[:, -1:, :].argmax(dim=-1),\\\n",
    "#     tokenizer(['A', 'B', 'C'])\n",
    "\n",
    "# tokenizer.batch_decode(res.logits[:, -1:, :].argmax(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
